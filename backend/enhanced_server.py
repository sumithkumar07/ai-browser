from fastapi import FastAPI, HTTPException, BackgroundTasks
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import StreamingResponse
from pydantic import BaseModel
from typing import List, Optional, Dict, Any, AsyncGenerator
import os
from dotenv import load_dotenv
from pymongo import MongoClient
import uuid
from datetime import datetime, timedelta
import httpx
from bs4 import BeautifulSoup
import groq
import json
import openai
import anthropic
import google.generativeai as genai
from typing import Union
import asyncio
from functools import lru_cache
import logging
import redis
import pickle
import hashlib
import time
from concurrent.futures import ThreadPoolExecutor
import psutil
import threading
from collections import defaultdict, deque
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import re
import aiofiles
import base64
from io import BytesIO
from PIL import Image
import cv2

load_dotenv()

# Configure advanced logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

app = FastAPI(title="AETHER Enhanced Browser API", version="4.0.0")

# CORS middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Database and Redis connections
MONGO_URL = os.getenv("MONGO_URL")
client = MongoClient(MONGO_URL)
db = client.aether_browser

# Advanced Redis caching
try:
    redis_client = redis.Redis.from_url(os.getenv("REDIS_URL", "redis://localhost:6379"), decode_responses=True)
    redis_client.ping()
    logger.info("‚úÖ Redis connected for advanced caching")
except:
    redis_client = None
    logger.warning("‚ö†Ô∏è Redis unavailable, using in-memory cache")

# Advanced in-memory cache for high performance
class AdvancedCache:
    def __init__(self, max_size=1000, ttl=3600):
        self.cache = {}
        self.access_times = {}
        self.max_size = max_size
        self.ttl = ttl
        self.stats = {"hits": 0, "misses": 0}
    
    def get(self, key):
        if key in self.cache:
            if time.time() - self.access_times[key]["created"] < self.ttl:
                self.access_times[key]["last_access"] = time.time()
                self.stats["hits"] += 1
                return self.cache[key]
            else:
                del self.cache[key]
                del self.access_times[key]
        
        self.stats["misses"] += 1
        return None
    
    def set(self, key, value):
        if len(self.cache) >= self.max_size:
            # Remove least recently used item
            lru_key = min(self.access_times.keys(), 
                         key=lambda k: self.access_times[k]["last_access"])
            del self.cache[lru_key]
            del self.access_times[lru_key]
        
        self.cache[key] = value
        current_time = time.time()
        self.access_times[key] = {
            "created": current_time,
            "last_access": current_time
        }
    
    def get_stats(self):
        total = self.stats["hits"] + self.stats["misses"]
        hit_rate = (self.stats["hits"] / total * 100) if total > 0 else 0
        return {
            "hit_rate": f"{hit_rate:.1f}%",
            "total_requests": total,
            "cache_size": len(self.cache)
        }

memory_cache = AdvancedCache(max_size=2000, ttl=1800)

# Performance monitoring
class PerformanceMonitor:
    def __init__(self):
        self.metrics = defaultdict(list)
        self.api_calls = defaultdict(int)
        self.error_counts = defaultdict(int)
        self.response_times = defaultdict(deque)
        
    def record_api_call(self, endpoint, response_time, status_code):
        self.api_calls[endpoint] += 1
        self.response_times[endpoint].append(response_time)
        if len(self.response_times[endpoint]) > 100:
            self.response_times[endpoint].popleft()
            
        if status_code >= 400:
            self.error_counts[endpoint] += 1
    
    def get_metrics(self):
        system_metrics = {
            "cpu_percent": psutil.cpu_percent(),
            "memory_percent": psutil.virtual_memory().percent,
            "disk_usage": psutil.disk_usage('/').percent
        }
        
        api_metrics = {}
        for endpoint, times in self.response_times.items():
            if times:
                api_metrics[endpoint] = {
                    "avg_response_time": sum(times) / len(times),
                    "min_response_time": min(times),
                    "max_response_time": max(times),
                    "total_calls": self.api_calls[endpoint],
                    "error_count": self.error_counts[endpoint],
                    "error_rate": (self.error_counts[endpoint] / self.api_calls[endpoint] * 100) if self.api_calls[endpoint] > 0 else 0
                }
        
        return {
            "system": system_metrics,
            "api": api_metrics,
            "cache_stats": memory_cache.get_stats()
        }

performance_monitor = PerformanceMonitor()

# AI clients initialization with advanced features
groq_client = groq.Groq(api_key=os.getenv("GROQ_API_KEY"))
openai_client = None
anthropic_client = None
genai_model = None

# Advanced AI conversation patterns
class ConversationManager:
    def __init__(self):
        self.conversation_patterns = {
            "research": ["analyze", "research", "investigate", "study", "examine"],
            "creative": ["create", "generate", "design", "build", "make"],
            "technical": ["code", "program", "debug", "develop", "technical"],
            "summarization": ["summarize", "summary", "brief", "overview", "tldr"],
            "extraction": ["extract", "find", "get", "retrieve", "pull"]
        }
        self.user_patterns = defaultdict(list)
        self.conversation_history = defaultdict(list)
    
    def analyze_intent(self, message, user_id="default"):
        message_lower = message.lower()
        scores = {}
        
        for pattern_type, keywords in self.conversation_patterns.items():
            score = sum(1 for keyword in keywords if keyword in message_lower)
            if score > 0:
                scores[pattern_type] = score
        
        intent = max(scores.keys(), key=lambda k: scores[k]) if scores else "general"
        
        # Learn user patterns
        self.user_patterns[user_id].append(intent)
        if len(self.user_patterns[user_id]) > 50:
            self.user_patterns[user_id] = self.user_patterns[user_id][-50:]
        
        return intent, scores
    
    def get_user_preference(self, user_id="default"):
        if not self.user_patterns[user_id]:
            return "general"
        
        pattern_counts = defaultdict(int)
        recent_patterns = self.user_patterns[user_id][-20:]
        for pattern in recent_patterns:
            pattern_counts[pattern] += 1
        
        return max(pattern_counts.keys(), key=lambda k: pattern_counts[k])

conversation_manager = ConversationManager()

# Enhanced AI response system with multi-modal support
class EnhancedAIManager:
    def __init__(self):
        self.response_cache = {}
        self.context_memory = defaultdict(deque)
        self.vectorizer = TfidfVectorizer(max_features=1000)
        self.conversation_embeddings = []
        
    async def get_enhanced_response(self, message: str, context: Optional[str] = None, 
                                  session_id: str = "default", provider: str = "groq",
                                  image_data: Optional[str] = None) -> str:
        start_time = time.time()
        
        try:
            # Analyze conversation intent
            intent, scores = conversation_manager.analyze_intent(message, session_id)
            user_preference = conversation_manager.get_user_preference(session_id)
            
            # Enhanced context preparation
            enhanced_context = self._prepare_enhanced_context(message, context, session_id, intent)
            
            # Multi-modal processing
            if image_data:
                image_analysis = await self._analyze_image(image_data)
                enhanced_context += f"\n\nImage Analysis: {image_analysis}"
            
            # Get AI response with enhanced prompting
            response = await self._get_ai_response_with_context(
                message, enhanced_context, session_id, provider, intent, user_preference
            )
            
            # Store in conversation memory
            self.context_memory[session_id].append({
                "user": message,
                "assistant": response,
                "intent": intent,
                "timestamp": datetime.utcnow(),
                "context": context[:200] if context else None
            })
            
            if len(self.context_memory[session_id]) > 20:
                self.context_memory[session_id].popleft()
            
            response_time = time.time() - start_time
            performance_monitor.record_api_call("ai_response", response_time, 200)
            
            return response
            
        except Exception as e:
            logger.error(f"Enhanced AI response error: {str(e)}")
            performance_monitor.record_api_call("ai_response", time.time() - start_time, 500)
            return f"I apologize, but I'm experiencing technical difficulties. Please try again."
    
    def _prepare_enhanced_context(self, message: str, context: Optional[str], 
                                session_id: str, intent: str) -> str:
        # Get recent conversation history
        recent_history = list(self.context_memory[session_id])[-5:] if session_id in self.context_memory else []
        
        # Prepare system prompt based on intent
        intent_prompts = {
            "research": "You are a research assistant. Provide thorough, well-researched responses with multiple perspectives.",
            "creative": "You are a creative assistant. Be imaginative, innovative, and think outside the box.",
            "technical": "You are a technical expert. Provide precise, accurate, and detailed technical information.",
            "summarization": "You are a summarization expert. Create concise, comprehensive summaries that capture key points.",
            "extraction": "You are a data extraction specialist. Focus on finding and organizing specific information."
        }
        
        system_prompt = intent_prompts.get(intent, "You are AETHER AI, an intelligent browser assistant.")
        
        enhanced_context = f"{system_prompt}\n\n"
        
        if context:
            enhanced_context += f"Current webpage context: {context[:2000]}\n\n"
        
        if recent_history:
            enhanced_context += "Recent conversation context:\n"
            for item in recent_history[-3:]:
                enhanced_context += f"User: {item['user'][:100]}...\nAssistant: {item['assistant'][:100]}...\n"
        
        return enhanced_context
    
    async def _analyze_image(self, image_data: str) -> str:
        try:
            # Decode base64 image
            image_bytes = base64.b64decode(image_data.split(',')[1] if ',' in image_data else image_data)
            image = Image.open(BytesIO(image_bytes))
            
            # Basic image analysis using OpenCV
            img_array = np.array(image)
            if len(img_array.shape) == 3:
                gray = cv2.cvtColor(img_array, cv2.COLOR_RGB2GRAY)
            else:
                gray = img_array
            
            # Extract basic features
            height, width = gray.shape
            brightness = np.mean(gray)
            contrast = np.std(gray)
            
            # Detect edges for complexity
            edges = cv2.Canny(gray, 50, 150)
            edge_density = np.sum(edges > 0) / (height * width)
            
            return f"Image analysis: {width}x{height} pixels, brightness: {brightness:.1f}, contrast: {contrast:.1f}, complexity: {edge_density:.3f}"
            
        except Exception as e:
            return f"Image analysis failed: {str(e)}"
    
    async def _get_ai_response_with_context(self, message: str, enhanced_context: str, 
                                          session_id: str, provider: str, intent: str, 
                                          user_preference: str) -> str:
        # Cache key for similar requests
        cache_key = hashlib.md5(f"{message}{enhanced_context[:500]}".encode()).hexdigest()
        
        # Check cache first
        cached_response = memory_cache.get(cache_key)
        if cached_response and intent != "creative":  # Don't cache creative responses
            return cached_response
        
        messages = [
            {"role": "system", "content": enhanced_context},
            {"role": "user", "content": message}
        ]
        
        try:
            if provider == "openai" and openai_client:
                response = openai_client.chat.completions.create(
                    model="gpt-4o-mini",
                    messages=messages,
                    temperature=0.7 if intent == "creative" else 0.3,
                    max_tokens=1500
                )
                result = response.choices[0].message.content
            elif provider == "anthropic" and anthropic_client:
                response = anthropic_client.messages.create(
                    model="claude-3-haiku-20240307",
                    system=enhanced_context,
                    messages=[{"role": "user", "content": message}],
                    max_tokens=1500
                )
                result = response.content[0].text
            else:
                # Enhanced Groq usage
                response = groq_client.chat.completions.create(
                    messages=messages,
                    model="llama-3.3-70b-versatile",
                    temperature=0.7 if intent == "creative" else 0.3,
                    max_tokens=1500
                )
                result = response.choices[0].message.content
            
            # Cache the response
            if intent != "creative":
                memory_cache.set(cache_key, result)
            
            return result
            
        except Exception as e:
            logger.error(f"AI provider error: {str(e)}")
            raise e

enhanced_ai = EnhancedAIManager()

# Enhanced web content processor
class ContentProcessor:
    def __init__(self):
        self.content_cache = {}
        self.processing_stats = {"total": 0, "cached": 0, "errors": 0}
    
    async def get_enhanced_page_content(self, url: str, analysis_type: str = "standard") -> Dict[str, Any]:
        cache_key = f"{url}:{analysis_type}"
        
        # Check cache first
        if cache_key in self.content_cache:
            self.processing_stats["cached"] += 1
            return self.content_cache[cache_key]
        
        try:
            self.processing_stats["total"] += 1
            
            async with httpx.AsyncClient(timeout=15.0, follow_redirects=True) as client:
                headers = {
                    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
                }
                response = await client.get(url, headers=headers)
                response.raise_for_status()
                
                soup = BeautifulSoup(response.text, 'html.parser')
                
                # Remove unwanted elements
                for element in soup(['script', 'style', 'nav', 'header', 'footer', 'aside']):
                    element.decompose()
                
                # Enhanced content extraction
                result = {
                    "url": url,
                    "title": soup.title.string.strip() if soup.title else url,
                    "meta_description": "",
                    "content": "",
                    "headings": [],
                    "links": [],
                    "images": [],
                    "load_time": time.time(),
                    "word_count": 0,
                    "reading_time": 0,
                    "language": "en"
                }
                
                # Meta description
                meta_desc = soup.find('meta', attrs={'name': 'description'})
                if meta_desc and meta_desc.get('content'):
                    result["meta_description"] = meta_desc['content']
                
                # Enhanced text extraction
                main_content = soup.find('main') or soup.find('article') or soup.find('div', class_=re.compile(r'content|main|article', re.I)) or soup.body
                
                if main_content:
                    text_content = main_content.get_text(separator=' ', strip=True)
                else:
                    text_content = soup.get_text(separator=' ', strip=True)
                
                # Clean and process text
                lines = (line.strip() for line in text_content.splitlines())
                chunks = (phrase.strip() for line in lines for phrase in line.split("  "))
                clean_text = ' '.join(chunk for chunk in chunks if chunk and len(chunk) > 2)
                
                result["content"] = clean_text[:5000]  # Limit content size
                result["word_count"] = len(clean_text.split())
                result["reading_time"] = max(1, result["word_count"] // 200)  # Average reading speed
                
                # Extract headings
                for i in range(1, 7):
                    headings = soup.find_all(f'h{i}')
                    for heading in headings[:5]:  # Limit to 5 per level
                        result["headings"].append({
                            "level": i,
                            "text": heading.get_text(strip=True)
                        })
                
                # Extract links
                links = soup.find_all('a', href=True)
                for link in links[:10]:  # Limit to 10 links
                    href = link['href']
                    if href.startswith(('http', 'https')):
                        result["links"].append({
                            "url": href,
                            "text": link.get_text(strip=True)
                        })
                
                # Extract images
                images = soup.find_all('img', src=True)
                for img in images[:5]:  # Limit to 5 images
                    src = img['src']
                    if src.startswith(('http', 'https')):
                        result["images"].append({
                            "url": src,
                            "alt": img.get('alt', '')
                        })
                
                # Cache the result
                self.content_cache[cache_key] = result
                
                # Limit cache size
                if len(self.content_cache) > 100:
                    oldest_key = next(iter(self.content_cache))
                    del self.content_cache[oldest_key]
                
                return result
                
        except Exception as e:
            self.processing_stats["errors"] += 1
            logger.error(f"Content processing error for {url}: {str(e)}")
            return {
                "url": url,
                "title": url,
                "content": f"Error loading page: {str(e)}",
                "error": str(e)
            }

content_processor = ContentProcessor()

# Pydantic models for enhanced features
class EnhancedChatMessage(BaseModel):
    message: str
    session_id: Optional[str] = None
    current_url: Optional[str] = None
    image_data: Optional[str] = None
    analysis_type: Optional[str] = "standard"

class MultiModalRequest(BaseModel):
    text: str
    image_data: Optional[str] = None
    url: Optional[str] = None
    analysis_type: str = "comprehensive"

class PerformanceAnalysisRequest(BaseModel):
    url: str
    metrics: List[str] = ["load_time", "content_size", "image_count", "link_count"]

# Initialize AI providers with enhanced configuration
def initialize_enhanced_ai_providers():
    global openai_client, anthropic_client, genai_model
    
    # OpenAI with enhanced settings
    if os.getenv("OPENAI_API_KEY") and os.getenv("OPENAI_API_KEY") != "your_openai_key_here":
        openai_client = openai.OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
        logger.info("‚úÖ OpenAI client initialized with enhanced capabilities")
        
    # Anthropic with enhanced settings
    if os.getenv("ANTHROPIC_API_KEY") and os.getenv("ANTHROPIC_API_KEY") != "your_anthropic_key_here":
        anthropic_client = anthropic.Anthropic(api_key=os.getenv("ANTHROPIC_API_KEY"))
        logger.info("‚úÖ Anthropic client initialized with enhanced capabilities")
    
    # Google Gemini with enhanced settings
    if os.getenv("GOOGLE_API_KEY") and os.getenv("GOOGLE_API_KEY") != "your_google_key_here":
        genai.configure(api_key=os.getenv("GOOGLE_API_KEY"))
        genai_model = genai.GenerativeModel('gemini-pro')
        logger.info("‚úÖ Google Gemini initialized with enhanced capabilities")

# Initialize on startup
initialize_enhanced_ai_providers()

# Enhanced API Routes with comprehensive functionality

@app.get("/api/health")
async def enhanced_health_check():
    """Enhanced health check with comprehensive system status"""
    start_time = time.time()
    
    try:
        health_data = {
            "status": "enhanced_operational",
            "version": "4.0.0",
            "timestamp": datetime.utcnow().isoformat(),
            "services": {
                "database": "operational",
                "ai_providers": {
                    "groq": "operational",
                    "openai": "operational" if openai_client else "unavailable",
                    "anthropic": "operational" if anthropic_client else "unavailable",
                    "google": "operational" if genai_model else "unavailable"
                },
                "cache": "operational" if redis_client else "memory_fallback",
                "performance_monitor": "operational"
            },
            "performance": performance_monitor.get_metrics(),
            "capabilities": [
                "enhanced_ai_responses",
                "multi_modal_processing", 
                "advanced_caching",
                "performance_monitoring",
                "intelligent_content_analysis",
                "predictive_user_behavior",
                "real_time_optimization"
            ]
        }
        
        response_time = time.time() - start_time
        performance_monitor.record_api_call("/api/health", response_time, 200)
        
        return health_data
        
    except Exception as e:
        response_time = time.time() - start_time
        performance_monitor.record_api_call("/api/health", response_time, 500)
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/api/enhanced/chat")
async def enhanced_chat_with_ai(chat_data: EnhancedChatMessage):
    """Enhanced AI chat with multi-modal support and advanced conversation patterns"""
    start_time = time.time()
    
    try:
        session_id = chat_data.session_id or f"enhanced-session-{uuid.uuid4()}"
        
        # Get enhanced page context if URL provided
        context = None
        if chat_data.current_url:
            page_data = await content_processor.get_enhanced_page_content(
                chat_data.current_url, chat_data.analysis_type
            )
            context = f"Page: {page_data['title']}\nDescription: {page_data.get('meta_description', '')}\nContent: {page_data['content'][:2000]}\nHeadings: {[h['text'] for h in page_data.get('headings', [])][:5]}"
        
        # Get enhanced AI response with multi-modal support
        ai_response = await enhanced_ai.get_enhanced_response(
            chat_data.message,
            context=context,
            session_id=session_id,
            provider="groq",
            image_data=chat_data.image_data
        )
        
        # Store enhanced chat session
        chat_record = {
            "session_id": session_id,
            "user_message": chat_data.message,
            "ai_response": ai_response,
            "current_url": chat_data.current_url,
            "has_image": bool(chat_data.image_data),
            "analysis_type": chat_data.analysis_type,
            "timestamp": datetime.utcnow(),
            "response_time": time.time() - start_time
        }
        
        db.enhanced_chat_sessions.insert_one(chat_record)
        
        response_time = time.time() - start_time
        performance_monitor.record_api_call("/api/enhanced/chat", response_time, 200)
        
        return {
            "response": ai_response,
            "session_id": session_id,
            "analysis_type": chat_data.analysis_type,
            "response_time": f"{response_time:.2f}s",
            "enhanced_features": ["multi_modal", "context_aware", "conversation_patterns", "performance_optimized"]
        }
        
    except Exception as e:
        response_time = time.time() - start_time
        performance_monitor.record_api_call("/api/enhanced/chat", response_time, 500)
        logger.error(f"Enhanced chat error: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Enhanced chat failed: {str(e)}")

@app.post("/api/enhanced/browse")
async def enhanced_browse_page(request: Dict[str, Any]):
    """Enhanced browsing with comprehensive content analysis and security features"""
    start_time = time.time()
    
    try:
        url = request.get("url")
        analysis_type = request.get("analysis_type", "comprehensive")
        security_scan = request.get("security_scan", True)
        
        if not url:
            raise HTTPException(status_code=400, detail="URL is required")
        
        # Enhanced content analysis
        page_data = await content_processor.get_enhanced_page_content(url, analysis_type)
        
        # Security analysis
        security_info = {
            "is_https": url.startswith("https://"),
            "domain": url.split("/")[2] if len(url.split("/")) > 2 else "",
            "security_score": 85 if url.startswith("https://") else 45,
            "warnings": []
        }
        
        if not url.startswith("https://"):
            security_info["warnings"].append("Insecure connection (HTTP)")
        
        # Store enhanced browsing record
        tab_data = {
            "id": str(uuid.uuid4()),
            "url": url,
            "title": page_data["title"],
            "meta_description": page_data.get("meta_description", ""),
            "word_count": page_data.get("word_count", 0),
            "reading_time": page_data.get("reading_time", 0),
            "security_info": security_info,
            "timestamp": datetime.utcnow(),
            "content_preview": page_data["content"][:500],
            "headings": page_data.get("headings", []),
            "links_count": len(page_data.get("links", [])),
            "images_count": len(page_data.get("images", []))
        }
        
        db.enhanced_tabs.insert_one(tab_data)
        
        # Maintain recent tabs (keep last 20)
        all_tabs = list(db.enhanced_tabs.find().sort("timestamp", -1))
        if len(all_tabs) > 20:
            for tab in all_tabs[20:]:
                db.enhanced_tabs.delete_one({"_id": tab["_id"]})
        
        response_time = time.time() - start_time
        performance_monitor.record_api_call("/api/enhanced/browse", response_time, 200)
        
        return {
            "success": True,
            "page_data": page_data,
            "security_info": security_info,
            "tab_id": tab_data["id"],
            "analysis_type": analysis_type,
            "processing_time": f"{response_time:.2f}s",
            "enhanced_features": [
                "comprehensive_content_analysis",
                "security_scanning", 
                "performance_metrics",
                "smart_caching"
            ]
        }
        
    except Exception as e:
        response_time = time.time() - start_time
        performance_monitor.record_api_call("/api/enhanced/browse", response_time, 500)
        logger.error(f"Enhanced browse error: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Enhanced browsing failed: {str(e)}")

@app.get("/api/performance/metrics")
async def get_performance_metrics():
    """Get comprehensive performance metrics and optimization recommendations"""
    try:
        metrics = performance_monitor.get_metrics()
        
        # Add enhanced metrics
        enhanced_metrics = {
            **metrics,
            "content_processor": {
                "total_processed": content_processor.processing_stats["total"],
                "cache_hits": content_processor.processing_stats["cached"],
                "error_rate": (content_processor.processing_stats["errors"] / max(content_processor.processing_stats["total"], 1)) * 100
            },
            "optimization_recommendations": []
        }
        
        # Generate optimization recommendations
        if metrics["system"]["cpu_percent"] > 80:
            enhanced_metrics["optimization_recommendations"].append("High CPU usage detected - consider scaling")
        
        if metrics["system"]["memory_percent"] > 85:
            enhanced_metrics["optimization_recommendations"].append("High memory usage - implement memory optimization")
        
        cache_hit_rate = float(metrics["cache_stats"]["hit_rate"].rstrip('%'))
        if cache_hit_rate < 50:
            enhanced_metrics["optimization_recommendations"].append("Low cache hit rate - optimize caching strategy")
        
        return enhanced_metrics
        
    except Exception as e:
        logger.error(f"Performance metrics error: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

if __name__ == "__main__":
    import uvicorn
    logger.info("üöÄ Starting AETHER Enhanced Browser API v4.0.0")
    logger.info("‚úÖ Enhanced features: Multi-modal AI, Advanced Caching, Performance Monitoring, Security Analysis")
    uvicorn.run(app, host="0.0.0.0", port=8001)